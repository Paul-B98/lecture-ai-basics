{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03054424",
   "metadata": {},
   "source": [
    "Lecture: AI I - Basics \n",
    "\n",
    "Previous:\n",
    "[**Chapter 4.2: Machine Learning with scikit-learn**](../04_ml/02_machine_learning.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e178e91",
   "metadata": {},
   "source": [
    "# Chapter 4.3: Evaluation with scikit-learn\n",
    "\n",
    "- [Cross-Validation](#cross-validation)\n",
    "- [Hyperparameter Tuning](#hyperparameter-tuning)\n",
    "- [Metrics](#metrics)\n",
    "- [Visualization of Results](#visualization-of-results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc024908",
   "metadata": {},
   "source": [
    "Model selection refers to choosing the ML model that best explains the given data. However, since there is a wide variety of different models, each with large parameter ranges, this task is very critical. For example, if a model is selected that performs very well on the training data but fails miserably on new data, it has not learned the underlying data distribution.  \n",
    "\n",
    "In this notebook, we will cover the following topics:  \n",
    "- Cross-validation  \n",
    "- Hyperparameter tuning  \n",
    "- Metrics and results  \n",
    "- Visualization of results  \n",
    "\n",
    "The topic of model selection and evaluation is also covered in great detail in the [sklearn user guide](https://scikit-learn.org/stable/model_selection.html).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2658296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5651cdff",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "Learning the parameters of a prediction function and testing on the same data is a methodological mistake: a model that simply repeats the labels of the samples it has just seen would achieve a perfect result, but would not be able to predict anything useful for unseen data. This situation is called overfitting. To avoid this, it is common practice in (supervised) machine learning experiments to set aside part of the available data as a test set.  \n",
    "\n",
    "### Simple data splitting\n",
    "To simply split the data, the [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) method can be used.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "213d947e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150,)\n",
      "(90, 4) (90,)\n",
      "(60, 4) (60,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load data\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# split the data into 2 sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e54915",
   "metadata": {},
   "source": [
    "Now the support vector machine can be trained with the training data and evaluated with the test data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7e680be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26666666666666666"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel='linear', C=0.001).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac5d365",
   "metadata": {},
   "source": [
    "Trying out different values for C.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de45eddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8833333333333333"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='linear', C=0.01).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fc7683e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='linear', C=0.1).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69c0ad8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd68d84",
   "metadata": {},
   "source": [
    "### Why Cross-validation?\n",
    "When evaluating different settings (\"hyperparameters\") for prediction functions, such as the C value that must be manually set for an SVM, there is still the risk of overfitting to the test set, since the parameters can be changed until the prediction functions work optimally. In this way, knowledge about the test set can \"leak\" into the model, and the evaluation metrics no longer reflect generalization performance. To solve this problem, another part of the dataset can be set aside as a so-called \"validation set\": training is done on the training set, evaluation is then performed on the validation set, and if the experiment seems successful, the final evaluation is carried out on the test set.  \n",
    "\n",
    "However, by splitting the available data into three sets, the number of data points that can be used to train the model is drastically reduced. In addition, the results may depend on a particular random choice of the (training, validation) set pair.  \n",
    "\n",
    "To overcome these problems, the cross-validation procedure can be applied. With this procedure, a test set is still needed, but the validation set is no longer necessary. In the basic version, the training set is split into $k$ sets during training. The algorithm is then trained $k$ times on $k-1$ sets. The evaluation of the model is performed on the $k$th set.  \n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\">  \n",
    "\n",
    "### Using Cross-validation\n",
    "The easiest way to use cross-validation is with the [cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score) helper function.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "458ed3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96666667, 1.        , 0.96666667, 0.96666667, 1.        ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "clf = svm.SVC(kernel='linear', C=1, random_state=42)\n",
    "\n",
    "k = 5\n",
    "scores = cross_val_score(clf, X, y, cv=k)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2032725c",
   "metadata": {},
   "source": [
    "The function returns an array of length $k$. Each element is the score for the respective subset.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be1a5b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98 accuracy with a standard deviation of 0.02\n"
     ]
    }
   ],
   "source": [
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42bb3dc",
   "metadata": {},
   "source": [
    "By default, the result of each cross-validation iteration is the value of the modelâ€™s `score` method. To change this, the `scoring` parameter can be set.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2306c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96658312, 1.        , 0.96658312, 0.96658312, 1.        ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(clf, X, y, cv=5, scoring='f1_macro')\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcda6799",
   "metadata": {},
   "source": [
    "Another way to perform cross-validation is offered by the [cross_validate](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html) method. In contrast to the `cross_val_score` function, multiple metrics can be passed to it. In addition to the results, the method also returns information about runtimes.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1101677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.00112128, 0.0012362 , 0.00085711, 0.00105786, 0.00059175]),\n",
       " 'score_time': array([0.00267792, 0.00205421, 0.00188398, 0.00191569, 0.00161552]),\n",
       " 'test_precision_macro': array([0.96969697, 1.        , 0.96969697, 0.96969697, 1.        ]),\n",
       " 'test_recall_macro': array([0.96666667, 1.        , 0.96666667, 0.96666667, 1.        ])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scoring = ['precision_macro', 'recall_macro']\n",
    "clf = svm.SVC(kernel='linear', C=1, random_state=0)\n",
    "scores = cross_validate(clf, X, y, scoring=scoring)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06652f4c",
   "metadata": {},
   "source": [
    "### Cross-validation Iterators\n",
    "Both functions can take either an integer value $k$ or a cross-validation iterator via the `cv` parameter. The first option results in the use of the [KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) or the [StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) iterator with $k$ folds. In the second case, the corresponding [cross-validation iterator](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators) is used to split the dataset during training.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bd0acd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Lecture: AI I - Basics \n",
    "\n",
    "Exercise: [**Exercise 4.3: Evaluation with scikit-learn**](../04_ml/exercises/03_evaluation.ipynb)\n",
    "\n",
    "Next: [**Chapter 5.1: Assesment 1**]()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-lecture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
