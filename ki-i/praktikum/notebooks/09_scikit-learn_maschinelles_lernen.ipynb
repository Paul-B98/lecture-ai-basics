{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "twenty-reference",
   "metadata": {},
   "source": [
    "Wahlpflichtfach Künstliche Intelligenz I: Praktikum \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-software",
   "metadata": {},
   "source": [
    "# 09 Scikit-learn - Maschinelles Lernen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-confidence",
   "metadata": {},
   "source": [
    "[Scikit-learn](https://scikit-learn.org/stable/index.html) bietet eine Vielzahl an Algorithmen zum maschinellen Lernen an. Da wir nicht die Zeit haben uns alle im Detail anzugucken, werden wir uns nur jeweils ein paar Algorithmen aus den Bereichen Klassifizierung, Regression und Clustering angucken. Für weiter Algorithmen verweisen wir an dieser Stelle auf die [Dokumentation](https://scikit-learn.org/stable/modules/classes.html), den [Benutzerleitfaden](https://scikit-learn.org/stable/user_guide.html) und die [Tutorials](https://scikit-learn.org/stable/tutorial/index.html). Außerdem können wir auch nicht auf die dahinterliegende Mathematik eingehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "headed-howard",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-portsmouth",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "[Decision Trees](https://scikit-learn.org/stable/modules/tree.html#tree) (Entscheidungsbäume) sind eine überwachte Lernmethode, die für Klassifizierung und Regression verwendet wird. Das Ziel ist es, ein Modell zu erstellen, das den Wert einer Zielvariablen durch das Lernen einfacher Entscheidungsregeln, die aus den Features abgeleitet werden, vorhersagt.\n",
    "\n",
    "__Vorteile:__\n",
    "- Einfach zu verstehen und zu interpretieren\n",
    "- Fast keine Datenvorverarbeitung notwendig\n",
    "\n",
    "__Nachteile:__\n",
    "- Eventuell keine gute Generalisierung (overfitting)\n",
    "- Entscheidungsbäume sind meistens sehr unstabil, da kleine Änderungen in den Daten eventuell einen vollkommen anderen Baum erzeugen\n",
    "\n",
    "### Klassifizierung mit Decision Trees\n",
    "Der [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) ist in der Lage, eine Mehrklassen-Klassifizierung für einen Datensatz durchzuführen. Dazu müssen an dem Objekt die Merhode `fit()`nur mit den Daten und Labels aufgerufen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-adult",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "dt_clf = tree.DecisionTreeClassifier()\n",
    "dt_clf.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-madonna",
   "metadata": {},
   "source": [
    "Anschließend ist es möglich Vorhersagen zu treffen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-console",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf.predict([[5.1 , 3.4, 1.3, 0.3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-reply",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf.predict([[6.5, 3.2 , 5.2 , 1.7],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-greenhouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf.predict([[6.4, 2.7, 5.3, 2.4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-philosophy",
   "metadata": {},
   "source": [
    "Um sich den Entscheidungsbaum anzeigen zu lassen kann die Funktion [plot_tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html#sklearn.tree.plot_tree) verwendet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "_ = tree.plot_tree(dt_clf, filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-mexico",
   "metadata": {},
   "source": [
    "Auch den Decision Tree kann man selbstverständlich parametrisieren. So kann zum Beispiel mit dem Parameter `max_depth` die maximale Tiefe des Baumes angegeben werden und mit dem Parameter `criterion` kann die Funktion zur Messung der Qualität eines Splits bestimmt werden. Je nach gewählten Parametern ergibt sich dann ein anderer Entscheidungsbaum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-thong",
   "metadata": {},
   "outputs": [],
   "source": [
    "different_dt_clf = tree.DecisionTreeClassifier(max_depth=3, criterion='entropy')\n",
    "different_dt_clf.fit(iris.data, iris.target)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "_ = tree.plot_tree(different_dt_clf, filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-sucking",
   "metadata": {},
   "source": [
    "### Regression mit Decision Trees\n",
    "Decision Trees können auch auf Regressionsproblemen angewendet werden. Dafür stellt sklearn die Klasse [DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor) bereit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "through-camera",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random dataset\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.sort(5 * rng.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel()\n",
    "y[::5] += 3 * (0.5 - rng.rand(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "constitutional-tsunami",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit two regression models\n",
    "regr_1 = tree.DecisionTreeRegressor(max_depth=2).fit(X, y)\n",
    "regr_2 = tree.DecisionTreeRegressor(max_depth=5).fit(X, y)\n",
    "\n",
    "# Predict\n",
    "X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n",
    "y_1 = regr_1.predict(X_test)\n",
    "y_2 = regr_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-comfort",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.scatter(X, y, s=20, edgecolor=\"black\",\n",
    "            c=\"darkorange\", label=\"training data\")\n",
    "plt.plot(X_test, y_1, color=\"cornflowerblue\",\n",
    "         label=\"max_depth=2\", linewidth=2)\n",
    "plt.plot(X_test, y_2, color=\"yellowgreen\", label=\"max_depth=5\", linewidth=2)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-malpractice",
   "metadata": {},
   "source": [
    "__Aufgaben:__\n",
    "<details>\n",
    "<summary>a) Welcher Regressor bildet die Daten besser ab und warum?</summary>\n",
    "Der Regressor mit einer maximalen Tiefe von 2 bildet die Daten besser ab, da der tiefere Baum overfitted. \n",
    "</details>\n",
    "<details>\n",
    "<summary>b) Was könnte ein Problem mit dem Decision Tree Regressor sein?</summary>\n",
    "Der Decision Tree Regressor bildet keinen kontunuierlichen Raum ab. Dadurch ist er für viele Regressionsprobleme eventuell nur bedingt zu gebrauchen.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-market",
   "metadata": {},
   "source": [
    "Den Entscheidungsbaum für Regressionsproblem kann man sich auch mit der Funktion `plot_tree` anzeigen lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-crest",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "_ = tree.plot_tree(regr_1, filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-bowling",
   "metadata": {},
   "source": [
    "Auch den Regressor kann man wieder mit verschiedenen Parametern anpassen. Zum Beispiel kann man mit `min_samples_leaf` angeben wie viele Datenpunkte mindestens in einem Blatt des Baumes sein müssen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-start",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_3 = tree.DecisionTreeRegressor(max_depth=5, min_samples_leaf=5).fit(X, y)\n",
    "y_3 = regr_3.predict(X_test)\n",
    "# Plot the results\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.scatter(X, y, s=20, edgecolor=\"black\",\n",
    "            c=\"darkorange\", label=\"training data\")\n",
    "for y_predicted, color, label in zip((y_1, y_2, y_3), \n",
    "                                     (\"cornflowerblue\", \"yellowgreen\", \"darkred\"), \n",
    "                                     (\"max_depth=2\", \"max_depth=5\", \"max_depth=5, min_samples_leaf=5\")):\n",
    "    plt.plot(X_test, y_predicted, color=color, label=label, linewidth=2)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-simulation",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "Der Random Forest ist ein Algorithmus der nach dem Perturb-and-Combine-Verfahren arbeitet. Das bedeutet, dass durch die Einführung von Zufälligkeit in die Klassifikatorenkonstruktion ein vielfältiger Satz von Klassifikatoren erzeugt wird. Die Vorhersage des Kollektives wird als die gemittelte Vorhersage der einzelnen Klassifikatoren angegeben.\n",
    "\n",
    "In einem Random Forest sind also viele Decision Trees gesammelt, die alle einen Wert hervorsagen. Im Anschluss werden dann all diese Vorhersagen genommen um eine einzelne Vorhersage zu treffen.\n",
    "\n",
    "Für die Klassifizierung wird die Klasse [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-northwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# construct a random forest with 3 trees and each tree is trained with maximal 80% of the data\n",
    "random_forest = RandomForestClassifier(n_estimators=3, max_samples=0.8).fit(iris.data, iris.target)\n",
    "random_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-university",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest.predict([[5.1 , 3.4, 1.3, 0.3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-municipality",
   "metadata": {},
   "source": [
    "Visualisierung der unterschiedlichen Entscheidungsbäume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-strap",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(3, figsize=(15, 30))\n",
    "for axis, dt in zip((ax1, ax2, ax3), random_forest.estimators_):\n",
    "    _ = tree.plot_tree(dt, filled=True, ax=axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-secondary",
   "metadata": {},
   "source": [
    "Auch für Regressionprobleme kann ein Random Forest verwendet werden. Dafür gibt es die Klasse [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-rates",
   "metadata": {},
   "source": [
    "## Support-Vektor-Maschinen\n",
    "[Support-Vektor-Maschinen](https://scikit-learn.org/stable/modules/svm.html) (SVMs) sind eine Reihe von überwachten Lernmethoden, die für Klassifizierung, Regression und Ausreißererkennung verwendet werden. \n",
    "\n",
    "__Vorteile:__\n",
    "- Effektiv in hochdimensionalen Räumen.\n",
    "- Immer noch effektiv in Fällen, in denen die Anzahl der Dimensionen größer ist als die Anzahl der Datenpunkte.\n",
    "- Sehr gute Speichereffizient, da nur ein Bruchteil der Trainingsdaten gespeichert wird (Support Vektoren)\n",
    "\n",
    "__Nachteile:__\n",
    "- SVMs geben Wahrscheinlichkeiten nicht direkt aus\n",
    "- Gefahr für overfitting, wenn die Anzahl der Feature viel größer ist als die Anzahl der Datenpunkte.\n",
    "\n",
    "### Klassifizierung mit Support-Vektor-Maschinen\n",
    "\n",
    "Im Falle der Klassifizierung versucht die SVM die Datenpunkte so in Klassen zu unterteilen, dass um die Klassengrenzen ein möglichst breiter Bereich frei von Objekten bleibt. In der nachfolgenden Abbildung unterteilen zwar beide Linien die Klassen, aber die Linie _A_ hat einen deutlich größeren Freiraum.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f2/Svm_intro.svg\">\n",
    "\n",
    "Zum Trennen der Daten wird je nach Daten ein andere Kernel benötigt. Im linken Teil der Abbildung sind die Klassen linear trennbar im rechten Teil hingegen nicht. Daher würde es dort auch keinen Sinn ergeben, einen linearer Kernel zu verwenden.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/de/a/a0/Diskriminanzfunktion.png\">\n",
    "\n",
    "In sklearn steht mit [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) eine Klasse zur Verfügung, die verschiedene Kernel implementiert. Eine ausführlichere Erklärung zu SVM und den unterschiedlichen Parametern finden sie [hier](https://www.ancud.de/support-vector-machine-svn/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bigger-incidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# we just use 2 dimensions here because we want to visualize the result\n",
    "X = iris.data[:, [2, 1]]\n",
    "y = iris.target\n",
    "\n",
    "models = (svm.SVC(kernel='linear', C=1), svm.SVC(kernel='rbf', C=1, gamma=.7))\n",
    "models = (clf.fit(X, y) for clf in models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "centered-reach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define helper functions\n",
    "def make_meshgrid(x, y, h=.02):\n",
    "    \"\"\"Create a mesh of points to plot in\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: data to base x-axis meshgrid on\n",
    "    y: data to base y-axis meshgrid on\n",
    "    h: stepsize for meshgrid, optional\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xx, yy : ndarray\n",
    "    \"\"\"\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    clf: a classifier\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-ethnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# title for the plots\n",
    "titles = ('SVC with linear kernel',\n",
    "          'SVC with RBF kernel')\n",
    "\n",
    "# Set-up 1x2 grid for plotting.\n",
    "fig, sub = plt.subplots(1, 2, figsize=(15, 7))\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "X0, X1 = X[:, 0], X[:, 1]\n",
    "xx, yy = make_meshgrid(X0, X1)\n",
    "\n",
    "for clf, title, ax in zip(models, titles, sub.flatten()):\n",
    "    plot_contours(ax, clf, xx, yy,\n",
    "                  cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xlabel('Petal length')\n",
    "    ax.set_ylabel('Sepal width')\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title(title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-timer",
   "metadata": {},
   "source": [
    "## k-means\n",
    "Der [KMeans]()-Algorithmus clustert Daten, indem er versucht, die Datenpunkte in $n$ Gruppen gleicher Varianz zu trennen und dabei ein Kriterium zu minimieren, das als Trägheit oder within-cluster sum-of-squares bekannt ist. Es handelt sich dabei um ein unüberwachtes Lernverfahren.\n",
    "\n",
    "$$\\sum_{i=0}^{n}min_{\\mu_j \\in C}(\\lVert x_i - \\mu_j \\rVert^2) $$ \n",
    "\n",
    "Grob durchläuft der Algorithmus dabei die folgenden Schritte:\n",
    "1. Auswahl der $n$ initialen Schwerpunkte $C$. Häufig sind dies einfach zufällige Punkte aus den Trainingsdaten.\n",
    "2. Aufteilung der Trainingsdaten auf die einzelnen Schwerpunkte anhand des obigen Kriteriums. \n",
    "3. Die neuen Clusterschwerpunkte berechnen. Diese ergeben sich aus dem Mittelwert aller Datenpunkte, die dem Cluster zugewiesen sind.\n",
    "4. Wenn die Veränderung der Schwerpunkte größer als ein Schwellenwert ist, Schritt 2 wiederholen. Wenn nicht, wurden die Clusterschwerpunkte gefunden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-roman",
   "metadata": {},
   "source": [
    "### Dummy-Daten erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "binding-humor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 50 samples for each of the 3 centers\n",
    "x, y = datasets.make_blobs(n_samples=1500, centers=3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-cliff",
   "metadata": {},
   "source": [
    "### Clustering mit KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-throat",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# create a KMeans object that searches for 3 clusters \n",
    "km_clustering = KMeans(n_clusters=3, random_state=42)\n",
    "km_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the clusters\n",
    "y_pred = km_clustering.fit_predict(x)\n",
    "\n",
    "# Visualize the result\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(x[:, 0], x[:, 1], c=y_pred)\n",
    "# plot centroids\n",
    "plt.scatter(km_clustering.cluster_centers_[:, 0], \n",
    "            km_clustering.cluster_centers_[:, 1], \n",
    "            marker='X', c='red', s=100)\n",
    "_ = plt.title(\"Vorhergesagte Cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-israeli",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check what happens if we take two instead of 3 clusters\n",
    "km_clustering = KMeans(n_clusters=2, random_state=42)\n",
    "y_pred = km_clustering.fit_predict(x)\n",
    "\n",
    "# Visualize the result\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(x[:, 0], x[:, 1], c=y_pred)\n",
    "# plot centroids\n",
    "plt.scatter(km_clustering.cluster_centers_[:, 0], \n",
    "            km_clustering.cluster_centers_[:, 1], \n",
    "            marker='X', c='red', s=100)\n",
    "_ = plt.title(\"Zu wenige Cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-license",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the data\n",
    "transformation = [[-0.50834549, 0.93667341], [0.30887718, -0.95253229]]\n",
    "x_aniso = np.dot(x, transformation)\n",
    "\n",
    "# let's fit our algorithm\n",
    "km_clustering = KMeans(n_clusters=3, random_state=42)\n",
    "y_pred = km_clustering.fit_predict(x_aniso)\n",
    "\n",
    "# Visualize the result\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(x_aniso[:, 0], x_aniso[:, 1], c=y_pred)\n",
    "# plot centroids\n",
    "plt.scatter(km_clustering.cluster_centers_[:, 0], \n",
    "            km_clustering.cluster_centers_[:, 1], \n",
    "            marker='X', c='red', s=100)\n",
    "_ = plt.title(\"Anisotropisch verteilte Cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-bristol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different variance\n",
    "x_varied, y_varied = datasets.make_blobs(n_samples=1500, cluster_std=[1.0, 3., 0.5], random_state=42)\n",
    "\n",
    "# let's fit our algorithm\n",
    "km_clustering = KMeans(n_clusters=3, random_state=42)\n",
    "y_pred = km_clustering.fit_predict(x_varied)\n",
    "\n",
    "# Visualize the result\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(x_varied[:, 0], x_varied[:, 1], c=y_pred)\n",
    "# plot centroids\n",
    "plt.scatter(km_clustering.cluster_centers_[:, 0], \n",
    "            km_clustering.cluster_centers_[:, 1], \n",
    "            marker='X', c='red', s=100)\n",
    "_ = plt.title(\"Unterschiedliche Varianz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-calibration",
   "metadata": {},
   "source": [
    "## Verwendung in Pipelines\n",
    "Selbstverständlich können die ML-Algorithmen auch zu einer Pipeline hinzugefügt werden. Dies geht allerdings nur, wenn der Algorithmus als letzter Schritt hinzugefügt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-residence",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# create some dummy data\n",
    "X, y = datasets.make_classification()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# First, scale the input, then, classify it with a SVM\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('svc', svm.SVC())])\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-pressing",
   "metadata": {},
   "source": [
    "## Generelle Funktionsweise\n",
    "Wie Ihr bestimmt bereits bemerkt habt sind die Schritte immer wieder die gleichen:\n",
    "1. __Konstruktor:__ Erstellen des ML-Algorithmus. Die meisten Algorithmen können in diesem Schritt parametrisiert werden.\n",
    "2. __fit():__ Training des ML-Algorithmus. Wenn es ein überwachts Lernverfahren ist, wird der Methode neben den Daten auch die Zielwerte (Label oder Zahlenwert) übergeben. Für ein unüberwachtes Lernverfahren werden nur die Daten übergeben.\n",
    "3. __predict():__ Mit der `predict()`-Methode kann der ML-Agorithmus anschließend Vorhesagen für die übergebenen Werte treffen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-cattle",
   "metadata": {},
   "source": [
    "## Welchen Algorithmus soll ich nehmen?\n",
    "Es gibt nicht den einen Algorithmus der für alle Anwendungsfälle geeignet ist. Welcher Algorithmus in Frage kommt hängt von der Art des Problems (Klassifizierung, Regression, Clustering), der Datenmenge, den Datentypen und noch weiteren Faktoren ab. Scikit-learn bietet die folgende [Übersicht](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) an, um einen ersten Ansatzpunkt zu haben. \n",
    "<img src=\"https://scikit-learn.org/stable/_downloads/b82bf6cd7438a351f19fac60fbc0d927/ml_map.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-eugene",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Wahlpflichtach Künstliche Intelligenz I: Praktikum"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
